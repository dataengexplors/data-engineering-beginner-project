# create code in jupyter (create environment and install dependencies)

# create bucket in s3

## create access key

access key: https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html

## give access to user to write in bucket

https://stackoverflow.com/questions/36272286/getting-access-denied-when-calling-the-putobject-operation-with-bucket-level-per

# setup env for airflow

## install docker desktop / docker

## deploy application locally in containers

https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

# create dag

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html

## change dockerfile to encompass dependency to write in s3

## re-deploy application locally in containers

- create dockerfile https://docs.docker.com/engine/reference/commandline/build/
- create again with steps and dockerfile
  https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

## test dag

## connect to
